{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9deede1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [[
    "# This script reads raw daily bar data, applies corporate action adjustments,\n",
    "# and writes the result to a partitioned Delta Lake table.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#\n",
    "# Part 1: Configuration & Setup\n",
    "# Define variables and retrieve the storage access key from Databricks Secrets.\n",
    "# This keeps our credentials secure and out of the notebook code.\n",
    "#\n",
    "\n",
    "# --- ACTION REQUIRED: Update with your storage account name ---\n",
    "storage_account_name = \"bedrockstorf29cbb33b8be\" \n",
    "\n",
    "# Configuration for secrets and containers\n",
    "secret_scope_name = \"bedrock-secrets\"\n",
    "secret_key_name = \"storage-account-key\"\n",
    "raw_data_container = \"raw-data\"\n",
    "refined_data_container = \"refined-data\"\n",
    "\n",
    "# Retrieve the secret key using the Databricks Utilities library\n",
    "storage_account_key = dbutils.secrets.get(scope=secret_scope_name, key=secret_key_name)\n",
    "\n"],
    ["# COMMAND ----------\n",
    "\n",
    "#\n",
    "# Part 2: Mount Data Lake Storage (Idempotent)\n",
    "# This cell creates mount points to easily access our blob storage containers\n",
    "# like a local file system. The checks make the cell safe to re-run.\n",
    "#\n",
    "\n",
    "mount_points = [mount.mountPoint for mount in dbutils.fs.mounts()]\n",
    "\n",
    "# Mount the raw data container if it's not already mounted\n",
    "raw_mount_point = f\"/mnt/{raw_data_container}\"\n",
    "if raw_mount_point not in mount_points:\n",
    "    print(f\"Mounting {raw_mount_point}...\")\n",
    "    dbutils.fs.mount(\n",
    "      source = f\"wasbs://{raw_data_container}@{storage_account_name}.blob.core.windows.net/\",\n",
    "      mount_point = raw_mount_point,\n",
    "      extra_configs = {f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": storage_account_key}\n",
    "    )\n",
    "    print(\"Mount successful.\")\n",
    "else:\n",
    "    print(f\"{raw_mount_point} is already mounted.\")\n",
    "\n",
    "# Mount the refined data container if it's not already mounted\n",
    "refined_mount_point = f\"/mnt/{refined_data_container}\"\n",
    "if refined_mount_point not in mount_points:\n",
    "    print(f\"Mounting {refined_mount_point}...\")\n",
    "    dbutils.fs.mount(\n",
    "      source = f\"wasbs://{refined_data_container}@{storage_account_name}.blob.core.windows.net/\",\n",
    "      mount_point = refined_mount_point,\n",
    "      extra_configs = {f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": storage_account_key}\n",
    "    )\n",
    "    print(\"Mount successful.\")\n",
    "else:\n",
    "    print(f\"{refined_mount_point} is already mounted.\")\n",
    "    \n",
    "# COMMAND ----------\n",
    "\n",
    "#\n",
    "# Part 3: Read Raw Data with a Strict Schema\n",
    "# We explicitly define the schema to ensure data quality and improve performance.\n",
    "#\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, LongType\n",
    "from pyspark.sql.functions import col\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# --- REVIEW NOTE: Corrected the schema to match the Alpaca API output exactly. ---\n",
    "# The raw ingestion script saves all columns from the Alpaca daily bars endpoint.\n",
    "# The schema must include all of them for a successful read.\n",
    "raw_data_schema = StructType([\n",
    "    StructField(\"t\", TimestampType(), True),    # Bar timestamp\n",
    "    StructField(\"o\", DoubleType(), True),     # Open\n",
    "    StructField(\"h\", DoubleType(), True),     # High\n",
    "    StructField(\"l\", DoubleType(), True),     # Low\n",
    "    StructField(\"c\", DoubleType(), True),     # Close\n",
    "    StructField(\"v\", LongType(), True),       # Volume\n",
    "    StructField(\"n\", LongType(), True),       # Number of trades\n",
    "    StructField(\"vw\", DoubleType(), True),    # Volume Weighted Average Price\n",
    "    StructField(\"ticker\", StringType(), True) # Ticker symbol we added\n",
    "])\n",
    "\n",
    "# Process yesterday's data\n",
    "ingestion_date = date.today() - timedelta(days=1)\n",
    "raw_data_path = f\"/mnt/{raw_data_container}/{ingestion_date.strftime('%Y/%m/%d')}/\"\n",
    "\n",
    "print(f\"Reading raw data from: {raw_data_path}\")\n",
    "\n",
    "# Read the data using the corrected schema and rename columns for clarity\n",
    "raw_df = spark.read.format(\"csv\") \\\n",
    "                .schema(raw_data_schema) \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .load(raw_data_path)\n",
    "\n",
    "bars_df = raw_df.withColumnRenamed(\"t\", \"timestamp\") \\\n",
    "                .withColumnRenamed(\"o\", \"open\") \\\n",
    "                .withColumnRenamed(\"h\", \"high\") \\\n",
    "                .withColumnRenamed(\"l\", \"low\") \\\n",
    "                .withColumnRenamed(\"c\", \"close\") \\\n",
    "                .withColumnRenamed(\"v\", \"volume\")\n",
    "\n",
    "print(\"Raw data read successfully:\")\n",
    "bars_df.printSchema()\n",
    "display(bars_df)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#\n",
    "# Part 4: The \"Refinery\" - Applying Corporate Action Adjustments\n",
    "# This is our key value-add. We simulate adjusting historical data for a stock split.\n",
    "#\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Create a mock Corporate Actions DataFrame for a historical Apple stock split\n",
    "split_data = [ (\"AAPL\", \"2020-08-31\", 4.0) ]\n",
    "split_df = spark.createDataFrame(split_data, [\"split_ticker\", \"split_date\", \"split_ratio\"])\n",
    "\n",
    "# Left join our market data with split data. We match on ticker and only\n",
    "# consider bars that occurred *before* the split date.\n",
    "adjusted_df = bars_df.join(\n",
    "    split_df,\n",
    "    (bars_df.ticker == split_df.split_ticker) & (bars_df.timestamp < col(\"split_date\")),\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# Apply adjustment logic. If a split occurred (split_ratio is not null),\n",
    "# divide prices and MULTIPLY volume to adjust for the split.\n",
    "# --- REVIEW NOTE: Added volume adjustment. This is critical for data accuracy. ---\n",
    "final_df = adjusted_df.withColumn(\n",
    "    \"open\",\n",
    "    when(col(\"split_ratio\").isNotNull(), col(\"open\") / col(\"split_ratio\")).otherwise(col(\"open\"))\n",
    ").withColumn(\n",
    "    \"high\",\n",
    "    when(col(\"split_ratio\").isNotNull(), col(\"high\") / col(\"split_ratio\")).otherwise(col(\"high\"))\n",
    ").withColumn(\n",
    "    \"low\",\n",
    "    when(col(\"split_ratio\").isNotNull(), col(\"low\") / col(\"split_ratio\")).otherwise(col(\"low\"))\n",
    ").withColumn(\n",
    "    \"close\",\n",
    "    when(col(\"split_ratio\").isNotNull(), col(\"close\") / col(\"split_ratio\")).otherwise(col(\"close\"))\n",
    ").withColumn(\n",
    "    \"volume\",\n",
    "    when(col(\"split_ratio\").isNotNull(), (col(\"volume\") * col(\"split_ratio\")).cast(LongType())).otherwise(col(\"volume\"))\n",
    ").select(\n",
    "    \"timestamp\", \"ticker\", \"open\", \"high\", \"low\", \"close\", \"volume\" # Select and re-order final columns\n",
    ")\n",
    "\n",
    "print(\"Data after applying adjustments:\")\n",
    "display(final_df)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#\n",
    "# Part 5: Write the Refined Data to Delta Lake\n",
    "# Save our clean, adjusted data in the Delta Lake format, partitioned by ticker\n",
    "# for high-performance queries.\n",
    "#\n",
    "\n",
    "refined_data_path = f\"/mnt/{refined_data_container}/daily_bars\"\n",
    "\n",
    "print(f\"Writing final data to Delta table at: {refined_data_path}\")\n",
    "\n",
    "final_df.write.format(\"delta\") \\\n",
    "          .mode(\"append\") \\\n",
    "          .partitionBy(\"ticker\") \\\n",
    "          .save(refined_data_path)\n",
    "\n",
    "print(\"Successfully wrote data to Delta Lake.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#\n",
    "# Part 6: Verification\n",
    "# Read the data back from the Delta table to confirm it was written correctly.\n",
    "#\n",
    "\n",
    "print(\"\\nVerifying data in the refined Delta table...\")\n",
    "refined_delta_df = spark.read.format(\"delta\").load(refined_data_path)\n",
    "\n",
    "refined_delta_df.printSchema()\n",
    "refined_delta_df.show(5)"
   ]]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
